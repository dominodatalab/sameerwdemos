{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae4a412f-bc84-470a-8c92-24c6ad2ebbe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47d86df3-4c72-4f0a-a886-c56ce9c2b0a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import mlflow\n",
    "import tensorflow\n",
    "from filelock import FileLock\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "import ray\n",
    "from ray import train, tune\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.air.integrations.keras import ReportCheckpointCallback\n",
    "#from domino_mlflow_utils.keras import ReportCheckpointCallback2\n",
    "from domino_mlflow_utils.mlflow_utilities import DominoMLflowUtilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d41ab95-8c80-498b-86f3-927abf2ed355",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6f444f2-4b23-4a0b-8e6f-bf401d181c50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "/mnt/data/MLFLOW_WANDB/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 00:26:44,483\tINFO packaging.py:530 -- Creating a file package for local directory '/mnt/code/domino_mlflow_utils'.\n",
      "2024-01-11 00:26:44,486\tINFO packaging.py:358 -- Pushing file package 'gcs://_ray_pkg_4c4e9e80ccc29da2.zip' (0.07MiB) to Ray cluster...\n",
      "2024-01-11 00:26:44,489\tINFO packaging.py:371 -- Successfully pushed file package 'gcs://_ray_pkg_4c4e9e80ccc29da2.zip'.\n",
      "SIGTERM handler is not set because current thread is not the main thread.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray Initializied\n",
      "Ray Host=ray-659f065bc6aa262f4114e8b6-ray-client.domino-compute.svc.cluster.local and Ray Port=10001\n"
     ]
    }
   ],
   "source": [
    "service_host = os.environ[\"RAY_HEAD_SERVICE_HOST\"]\n",
    "service_port = os.environ[\"RAY_HEAD_SERVICE_PORT\"]\n",
    "print(ray.is_initialized())\n",
    "\n",
    "if not ray.is_initialized():\n",
    "\n",
    "    address=f\"ray://{service_host}:{service_port}\"\n",
    "    temp_dir='/mnt/data/{}/'.format(os.environ['DOMINO_PROJECT_NAME']) #set to a dataset\n",
    "    print(temp_dir)\n",
    "    ray.init(address=address, _temp_dir=temp_dir, runtime_env={\"py_modules\": ['/mnt/code/domino_mlflow_utils']})\n",
    "\n",
    "print('Ray Initializied')\n",
    "print(f'Ray Host={service_host} and Ray Port={service_port}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2b76036-45ed-483f-b249-7a7828d3a24b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRAC-GPU-TEST-sameer_wadkar-MLFLOW_WANDB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/mlflow', creation_time=1704929421852, experiment_id='452', last_update_time=1704929421852, lifecycle_stage='active', name='FRAC-GPU-TEST-sameer_wadkar-MLFLOW_WANDB', tags={'mlflow.domino.dataset_info': '65660f8009b68e2720e6c80c-65660f8009b68e2720e6c80b',\n",
       " 'mlflow.domino.environment_id': '659ee75412b6f32d73f252e0',\n",
       " 'mlflow.domino.environment_revision_id': '659f02e8c6aa262f4114e8af',\n",
       " 'mlflow.domino.hardware_tier': 'small-k8s',\n",
       " 'mlflow.domino.project_id': '65660f7f09b68e2720e6c808',\n",
       " 'mlflow.domino.project_name': 'MLFLOW_WANDB',\n",
       " 'mlflow.domino.run_id': '659f065bc6aa262f4114e8b6',\n",
       " 'mlflow.domino.run_number': '14',\n",
       " 'mlflow.domino.user': 'sameer_wadkar',\n",
       " 'mlflow.domino.user_id': '6283a3966d4fd0362f8ba2a8',\n",
       " 'mlflow.source.type': 'NOTEBOOK',\n",
       " 'mlflow.user': 'sameer_wadkar'}>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_name = 'FRAC-GPU-TEST'+'-' + os.environ['DOMINO_STARTING_USERNAME'] + '-' + os.environ['DOMINO_PROJECT_NAME']\n",
    "mlflow_tracking_uri = os.environ['CLUSTER_MLFLOW_TRACKING_URI']\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment = client.get_experiment_by_name(name=experiment_name)\n",
    "if(experiment is None):\n",
    "    print('Creating experiment ')\n",
    "    client.create_experiment(name=experiment_name)\n",
    "    experiment = client.get_experiment_by_name(name=experiment_name)\n",
    "\n",
    "print(experiment_name)\n",
    "mlflow.set_experiment(experiment_name=experiment_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4c81821-8b4f-4f32-b261-323da3c483b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray.air._internal.mlflow import _MLflowLoggerUtil\n",
    "mlflow_util = _MLflowLoggerUtil()\n",
    "def initialize_run():    \n",
    "    experiment_name = 'FRAC-GPU-TEST'+'-' + os.environ['DOMINO_STARTING_USERNAME'] + '-' + os.environ['DOMINO_PROJECT_NAME']\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    mlflow_tracking_uri = os.environ['CLUSTER_MLFLOW_TRACKING_URI']\n",
    "\n",
    "    mlflow_util.setup_mlflow(\n",
    "            tracking_uri=mlflow_tracking_uri,            \n",
    "            experiment_name=experiment_name,\n",
    "        )\n",
    "    now = round(time.time())\n",
    "    now_str=time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(now))\n",
    "\n",
    "\n",
    "\n",
    "    mlflow_util.start_run(tags={}, run_name=f\"root-{now_str}\")\n",
    "    return run.info.run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a86210d2-b6c9-4e08-b6e4-de5ac860bd03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "temp_dir='/mnt/data/{}/'.format(os.environ['DOMINO_PROJECT_NAME']) #set to a dataset\n",
    "experiment_name = 'FRAC-GPU-TEST'+'-' + os.environ['DOMINO_STARTING_USERNAME'] + '-' + os.environ['DOMINO_PROJECT_NAME']\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "mlflow_tracking_uri = os.environ['CLUSTER_MLFLOW_TRACKING_URI']\n",
    "\n",
    "def train_mnist(config):\n",
    "    # https://github.com/tensorflow/tensorflow/issues/32159\n",
    "    \n",
    "\n",
    "    batch_size = 128\n",
    "    num_classes = 10\n",
    "    epochs = 12\n",
    "    parent_run_id = config['parent_run_id']\n",
    "\n",
    "    with FileLock(os.path.expanduser(\"~/.data.lock\")):\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    model = tf.keras.models.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(config[\"hidden\"], activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=tf.keras.optimizers.SGD(lr=config[\"lr\"], momentum=config[\"momentum\"]),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    #cb = ReportCheckpointCallback2(metrics={\"mean_accuracy\": \"accuracy\"})\n",
    "    \n",
    "    mlflow.tensorflow.autolog()\n",
    "    run_tags={}\n",
    "    run_tags[\"mlflow.parentRunId\"] = parent_run_id\n",
    "    mlflow_utils = DominoMLflowUtilities()    \n",
    "    mlflow_utils.init(experiment_name,config,run_tags=run_tags)\n",
    "\n",
    "    model.fit(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=0,\n",
    "            validation_data=(x_test, y_test),\n",
    "            callbacks=[ReportCheckpointCallback(metrics={\"mean_accuracy\": \"accuracy\"})],\n",
    "        )\n",
    "    mlflow_utils.finish()\n",
    "\n",
    "\n",
    "def tune_mnist(parent_run_id):\n",
    "    sched = AsyncHyperBandScheduler(\n",
    "        time_attr=\"training_iteration\", max_t=400, grace_period=20\n",
    "    )\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(train_mnist, resources={\"cpu\": 1, \"gpu\": 0}),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"mean_accuracy\",\n",
    "            mode=\"max\",\n",
    "            scheduler=sched,\n",
    "            num_samples=10,            \n",
    "        ),\n",
    "        run_config=train.RunConfig(\n",
    "            name=\"exp\",\n",
    "            stop={\"mean_accuracy\": 0.99},\n",
    "            storage_path=temp_dir,\n",
    "        ),\n",
    "        param_space={\n",
    "            \"threads\": 2,\n",
    "            \"lr\": tune.uniform(0.001, 0.1),\n",
    "            \"momentum\": tune.uniform(0.1, 0.9),\n",
    "            \"hidden\": tune.randint(32, 512),\n",
    "            \"parent_run_id\": parent_run_id\n",
    "        },\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "    \n",
    "    print(\"Best hyperparameters found were: \", results.get_best_result().config)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "267363c7-103f-4eae-8d2e-379de6762220",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╭──────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ Configuration for experiment     exp                     │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ├──────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ Search algorithm                 BasicVariantGenerator   │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ Scheduler                        AsyncHyperBandScheduler │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ Number of trials                 10                      │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╰──────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m View detailed results here: /mnt/data/MLFLOW_WANDB/exp\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m To visualize your results with TensorBoard, run: `tensorboard --logdir /home/ray/ray_results/exp`\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial status: 10 PENDING\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Current time: 2024-01-10 16:27:00. Total running time: 0s\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Logical resource usage: 0/6 CPUs, 0/0 GPUs\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╭────────────────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ Trial name                status            lr     momentum     hidden │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ├────────────────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00000   PENDING    0.044958      0.164275        457 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00001   PENDING    0.0531664     0.551038        142 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00002   PENDING    0.0483602     0.860322        168 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00003   PENDING    0.0819864     0.769916        119 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00004   PENDING    0.0329543     0.512749        410 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00005   PENDING    0.0586611     0.722196        202 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00006   PENDING    0.0604956     0.183346        232 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00007   PENDING    0.0474074     0.600219        227 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00008   PENDING    0.0338391     0.29948         394 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00009   PENDING    0.0658625     0.324079        343 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╰────────────────────────────────────────────────────────────────────────╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.\n",
      "\u001b[36m(pid=2689, ip=10.0.114.178)\u001b[0m 2024-01-10 16:27:03.542076: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[36m(pid=3516, ip=10.0.63.23)\u001b[0m 2024-01-10 16:27:04.042724: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[36m(pid=3214, ip=10.0.108.213)\u001b[0m 2024-01-10 16:27:04.151946: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[36m(pid=3244, ip=10.0.108.154)\u001b[0m 2024-01-10 16:27:04.130871: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[36m(pid=3415, ip=10.0.100.59)\u001b[0m 2024-01-10 16:27:04.271023: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial train_mnist_20682_00000 started with configuration:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╭───────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ Trial train_mnist_20682_00000 config                          │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ├───────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ hidden                                                    457 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ lr                                                    0.04496 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ momentum                                              0.16427 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ parent_run_id                            ...4a2bb4feb4d87b362 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ threads                                                     2 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╰───────────────────────────────────────────────────────────────╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_mnist pid=2689, ip=10.0.114.178)\u001b[0m 2024-01-10 16:27:05.942997: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "\u001b[36m(train_mnist pid=2689, ip=10.0.114.178)\u001b[0m 2024-01-10 16:27:05.943051: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[36m(train_mnist pid=2689, ip=10.0.114.178)\u001b[0m 2024-01-10 16:27:05.943067: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ray-659f065bc6aa262f4114e8b6-ray-worker-2): /proc/driver/nvidia/version does not exist\n",
      "\u001b[36m(train_mnist pid=2689, ip=10.0.114.178)\u001b[0m 2024-01-10 16:27:05.943268: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "\u001b[36m(train_mnist pid=2689, ip=10.0.114.178)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(train_mnist pid=2689, ip=10.0.114.178)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "\u001b[36m(train_mnist pid=2689, ip=10.0.114.178)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_mnist pid=2689, ip=10.0.114.178)\u001b[0m 2024/01/10 16:27:06 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of tensorflow. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a supported version, or try upgrading MLflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial train_mnist_20682_00000 errored after 0 iterations at 2024-01-10 16:27:06. Total running time: 5s\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Error file: /home/ray/ray_results/exp/train_mnist_20682_00000_0_hidden=457,lr=0.0450,momentum=0.1643_2024-01-10_16-27-00/error.txt\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial train_mnist_20682_00003 started with configuration:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╭───────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ Trial train_mnist_20682_00003 config                          │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ├───────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ hidden                                                    119 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ lr                                                    0.08199 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ momentum                                              0.76992 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ parent_run_id                            ...4a2bb4feb4d87b362 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ threads                                                     2 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╰───────────────────────────────────────────────────────────────╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial task failed for trial train_mnist_20682_00000\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     result = ray.get(future)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/worker.py\", line 2626, in get\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise value\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tclass_name: ImplicitFunc\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tactor_id: a922d1b5638940f014f6253516000000\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tpid: 2689\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tnamespace: 53bc2fbb-d6bc-49b8-94e6-19df21a51234\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tip: 10.0.114.178\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None. The worker may have exceeded K8s pod memory limits. Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1807, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1908, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1813, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1754, in ray._raylet.execute_task.function_executor\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise skipped from exception_cause(skipped)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/air/_internal/util.py\", line 88, in run\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     self._ret = self._target(*self._args, **self._kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 115, in <lambda>\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     training_func=lambda: self._trainable_func(self.config),\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 332, in _trainable_func\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     output = fn()\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/tmp/ipykernel_2843/3889220660.py\", line 39, in train_mnist\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m TypeError: init() got an unexpected keyword argument 'tags'\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 733, in dump\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return Pickler.dump(self, obj)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 826, in reducer_override\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     if sys.version_info[:2] < (3, 7) and _is_parametrized_type_hint(\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m RecursionError: maximum recursion depth exceeded in comparison\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2206, in ray._raylet.task_execution_handler\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2102, in ray._raylet.execute_task_with_cancellation_handler\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1756, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1757, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1995, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1085, in ray._raylet.store_task_errors\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 4377, in ray._raylet.CoreWorker.store_task_outputs\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 494, in serialize\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return self._serialize_to_msgpack(value)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 449, in _serialize_to_msgpack\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     value = value.to_bytes()\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/exceptions.py\", line 32, in to_bytes\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     serialized_exception=pickle.dumps(self),\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 88, in dumps\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     cp.dump(obj)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 739, in dump\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise pickle.PicklingError(msg) from e\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m _pickle.PicklingError: Could not pickle object as excessively deep recursion required.\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m An unexpected internal error occurred while the worker was executing a task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial train_mnist_20682_00001 started with configuration:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╭───────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ Trial train_mnist_20682_00001 config                          │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ├───────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ hidden                                                    142 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ lr                                                    0.05317 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ momentum                                              0.55104 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ parent_run_id                            ...4a2bb4feb4d87b362 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ threads                                                     2 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╰───────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial train_mnist_20682_00002 started with configuration:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╭───────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ Trial train_mnist_20682_00002 config                          │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ├───────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ hidden                                                    168 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ lr                                                    0.04836 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ momentum                                              0.86032 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ parent_run_id                            ...4a2bb4feb4d87b362 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ threads                                                     2 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╰───────────────────────────────────────────────────────────────╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_mnist pid=3516, ip=10.0.63.23)\u001b[0m 2024-01-10 16:27:06.698558: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "\u001b[36m(train_mnist pid=3516, ip=10.0.63.23)\u001b[0m 2024-01-10 16:27:06.698592: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[36m(train_mnist pid=3516, ip=10.0.63.23)\u001b[0m 2024-01-10 16:27:06.698609: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ray-659f065bc6aa262f4114e8b6-ray-worker-3): /proc/driver/nvidia/version does not exist\n",
      "\u001b[36m(train_mnist pid=3516, ip=10.0.63.23)\u001b[0m 2024-01-10 16:27:06.698895: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "\u001b[36m(train_mnist pid=3516, ip=10.0.63.23)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(train_mnist pid=3516, ip=10.0.63.23)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "\u001b[36m(train_mnist pid=3516, ip=10.0.63.23)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_mnist pid=3516, ip=10.0.63.23)\u001b[0m 2024/01/10 16:27:06 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of tensorflow. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a supported version, or try upgrading MLflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial train_mnist_20682_00004 started with configuration:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╭───────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ Trial train_mnist_20682_00004 config                          │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ├───────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ hidden                                                    410 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ lr                                                    0.03295 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ momentum                                              0.51275 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ parent_run_id                            ...4a2bb4feb4d87b362 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ threads                                                     2 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╰───────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial train_mnist_20682_00003 errored after 0 iterations at 2024-01-10 16:27:06. Total running time: 6s\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Error file: /home/ray/ray_results/exp/train_mnist_20682_00003_3_hidden=119,lr=0.0820,momentum=0.7699_2024-01-10_16-27-00/error.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial task failed for trial train_mnist_20682_00003\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     result = ray.get(future)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/worker.py\", line 2626, in get\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise value\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tclass_name: ImplicitFunc\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tactor_id: 10b91af28e92e9c188cbbe4a16000000\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tpid: 3516\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tnamespace: 53bc2fbb-d6bc-49b8-94e6-19df21a51234\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tip: 10.0.63.23\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None. The worker may have exceeded K8s pod memory limits. Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1807, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1908, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1813, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1754, in ray._raylet.execute_task.function_executor\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise skipped from exception_cause(skipped)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/air/_internal/util.py\", line 88, in run\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     self._ret = self._target(*self._args, **self._kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 115, in <lambda>\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     training_func=lambda: self._trainable_func(self.config),\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 332, in _trainable_func\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     output = fn()\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/tmp/ipykernel_2843/3889220660.py\", line 39, in train_mnist\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m TypeError: init() got an unexpected keyword argument 'tags'\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 733, in dump\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return Pickler.dump(self, obj)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 826, in reducer_override\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     if sys.version_info[:2] < (3, 7) and _is_parametrized_type_hint(\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m RecursionError: maximum recursion depth exceeded in comparison\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2206, in ray._raylet.task_execution_handler\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2102, in ray._raylet.execute_task_with_cancellation_handler\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1756, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1757, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1995, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1085, in ray._raylet.store_task_errors\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 4377, in ray._raylet.CoreWorker.store_task_outputs\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 494, in serialize\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return self._serialize_to_msgpack(value)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 449, in _serialize_to_msgpack\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     value = value.to_bytes()\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/exceptions.py\", line 32, in to_bytes\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     serialized_exception=pickle.dumps(self),\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 88, in dumps\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     cp.dump(obj)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 739, in dump\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise pickle.PicklingError(msg) from e\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m _pickle.PicklingError: Could not pickle object as excessively deep recursion required.\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m An unexpected internal error occurred while the worker was executing a task.\n",
      "\u001b[36m(train_mnist pid=3214, ip=10.0.108.213)\u001b[0m 2024-01-10 16:27:07.334605: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "\u001b[36m(train_mnist pid=3214, ip=10.0.108.213)\u001b[0m 2024-01-10 16:27:07.334649: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[36m(train_mnist pid=3214, ip=10.0.108.213)\u001b[0m 2024-01-10 16:27:07.334677: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ray-659f065bc6aa262f4114e8b6-ray-worker-0): /proc/driver/nvidia/version does not exist\n",
      "\u001b[36m(train_mnist pid=3214, ip=10.0.108.213)\u001b[0m 2024-01-10 16:27:07.334956: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "\u001b[36m(train_mnist pid=3214, ip=10.0.108.213)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(train_mnist pid=3214, ip=10.0.108.213)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "\u001b[36m(train_mnist pid=3214, ip=10.0.108.213)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_mnist pid=3244, ip=10.0.108.154)\u001b[0m 2024-01-10 16:27:07.399074: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "\u001b[36m(train_mnist pid=3244, ip=10.0.108.154)\u001b[0m 2024-01-10 16:27:07.399137: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[36m(train_mnist pid=3244, ip=10.0.108.154)\u001b[0m 2024-01-10 16:27:07.399173: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ray-659f065bc6aa262f4114e8b6-ray-worker-4): /proc/driver/nvidia/version does not exist\n",
      "\u001b[36m(train_mnist pid=3244, ip=10.0.108.154)\u001b[0m 2024-01-10 16:27:07.399494: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "\u001b[36m(train_mnist pid=3244, ip=10.0.108.154)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(train_mnist pid=3244, ip=10.0.108.154)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "\u001b[36m(train_mnist pid=3244, ip=10.0.108.154)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_mnist pid=3244, ip=10.0.108.154)\u001b[0m 2024/01/10 16:27:07 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of tensorflow. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a supported version, or try upgrading MLflow.\n",
      "\u001b[36m(pid=15682)\u001b[0m 2024-01-10 16:27:07.445831: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[36m(train_mnist pid=3214, ip=10.0.108.213)\u001b[0m 2024/01/10 16:27:07 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of tensorflow. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a supported version, or try upgrading MLflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial train_mnist_20682_00002 errored after 0 iterations at 2024-01-10 16:27:07. Total running time: 6s\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Error file: /home/ray/ray_results/exp/train_mnist_20682_00002_2_hidden=168,lr=0.0484,momentum=0.8603_2024-01-10_16-27-00/error.txt\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial train_mnist_20682_00001 errored after 0 iterations at 2024-01-10 16:27:07. Total running time: 6s\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Error file: /home/ray/ray_results/exp/train_mnist_20682_00001_1_hidden=142,lr=0.0532,momentum=0.5510_2024-01-10_16-27-00/error.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial task failed for trial train_mnist_20682_00002\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     result = ray.get(future)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/worker.py\", line 2626, in get\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise value\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tclass_name: ImplicitFunc\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tactor_id: 3a4df2cc388dc3c95633ebdb16000000\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tpid: 3244\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tnamespace: 53bc2fbb-d6bc-49b8-94e6-19df21a51234\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tip: 10.0.108.154\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None. The worker may have exceeded K8s pod memory limits. Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1807, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1908, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1813, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1754, in ray._raylet.execute_task.function_executor\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise skipped from exception_cause(skipped)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/air/_internal/util.py\", line 88, in run\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     self._ret = self._target(*self._args, **self._kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 115, in <lambda>\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     training_func=lambda: self._trainable_func(self.config),\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 332, in _trainable_func\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     output = fn()\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/tmp/ipykernel_2843/3889220660.py\", line 39, in train_mnist\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m TypeError: init() got an unexpected keyword argument 'tags'\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 733, in dump\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return Pickler.dump(self, obj)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 826, in reducer_override\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     if sys.version_info[:2] < (3, 7) and _is_parametrized_type_hint(\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m RecursionError: maximum recursion depth exceeded in comparison\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2206, in ray._raylet.task_execution_handler\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2102, in ray._raylet.execute_task_with_cancellation_handler\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1756, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1757, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1995, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1085, in ray._raylet.store_task_errors\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 4377, in ray._raylet.CoreWorker.store_task_outputs\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 494, in serialize\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return self._serialize_to_msgpack(value)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 449, in _serialize_to_msgpack\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     value = value.to_bytes()\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/exceptions.py\", line 32, in to_bytes\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     serialized_exception=pickle.dumps(self),\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 88, in dumps\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     cp.dump(obj)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 739, in dump\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise pickle.PicklingError(msg) from e\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m _pickle.PicklingError: Could not pickle object as excessively deep recursion required.\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m An unexpected internal error occurred while the worker was executing a task.\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial task failed for trial train_mnist_20682_00001\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     result = ray.get(future)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/worker.py\", line 2626, in get\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise value\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tclass_name: ImplicitFunc\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tactor_id: 1f27132f56229f2b1288a80d16000000\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tpid: 3214\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tnamespace: 53bc2fbb-d6bc-49b8-94e6-19df21a51234\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tip: 10.0.108.213\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None. The worker may have exceeded K8s pod memory limits. Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1807, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1908, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1813, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1754, in ray._raylet.execute_task.function_executor\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise skipped from exception_cause(skipped)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/air/_internal/util.py\", line 88, in run\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     self._ret = self._target(*self._args, **self._kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 115, in <lambda>\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     training_func=lambda: self._trainable_func(self.config),\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 332, in _trainable_func\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     output = fn()\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/tmp/ipykernel_2843/3889220660.py\", line 39, in train_mnist\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m TypeError: init() got an unexpected keyword argument 'tags'\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 733, in dump\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return Pickler.dump(self, obj)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 826, in reducer_override\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     if sys.version_info[:2] < (3, 7) and _is_parametrized_type_hint(\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m RecursionError: maximum recursion depth exceeded in comparison\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2206, in ray._raylet.task_execution_handler\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2102, in ray._raylet.execute_task_with_cancellation_handler\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1756, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1757, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1995, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1085, in ray._raylet.store_task_errors\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 4377, in ray._raylet.CoreWorker.store_task_outputs\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 494, in serialize\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return self._serialize_to_msgpack(value)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 449, in _serialize_to_msgpack\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     value = value.to_bytes()\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/exceptions.py\", line 32, in to_bytes\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     serialized_exception=pickle.dumps(self),\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 88, in dumps\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     cp.dump(obj)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 739, in dump\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise pickle.PicklingError(msg) from e\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m _pickle.PicklingError: Could not pickle object as excessively deep recursion required.\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m An unexpected internal error occurred while the worker was executing a task.\n",
      "\u001b[36m(train_mnist pid=3415, ip=10.0.100.59)\u001b[0m 2024-01-10 16:27:07.611249: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "\u001b[36m(train_mnist pid=3415, ip=10.0.100.59)\u001b[0m 2024-01-10 16:27:07.611287: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[36m(train_mnist pid=3415, ip=10.0.100.59)\u001b[0m 2024-01-10 16:27:07.611308: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ray-659f065bc6aa262f4114e8b6-ray-worker-1): /proc/driver/nvidia/version does not exist\n",
      "\u001b[36m(train_mnist pid=3415, ip=10.0.100.59)\u001b[0m 2024-01-10 16:27:07.611516: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "\u001b[36m(train_mnist pid=3415, ip=10.0.100.59)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(train_mnist pid=3415, ip=10.0.100.59)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "\u001b[36m(train_mnist pid=3415, ip=10.0.100.59)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_mnist pid=3415, ip=10.0.100.59)\u001b[0m 2024/01/10 16:27:07 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of tensorflow. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a supported version, or try upgrading MLflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial train_mnist_20682_00004 errored after 0 iterations at 2024-01-10 16:27:07. Total running time: 7s\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Error file: /home/ray/ray_results/exp/train_mnist_20682_00004_4_hidden=410,lr=0.0330,momentum=0.5127_2024-01-10_16-27-00/error.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial task failed for trial train_mnist_20682_00004\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     result = ray.get(future)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/worker.py\", line 2626, in get\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise value\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tclass_name: ImplicitFunc\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tactor_id: 22330291dbab1c1b8098280e16000000\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tpid: 3415\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tnamespace: 53bc2fbb-d6bc-49b8-94e6-19df21a51234\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tip: 10.0.100.59\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None. The worker may have exceeded K8s pod memory limits. Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1807, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1908, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1813, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1754, in ray._raylet.execute_task.function_executor\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise skipped from exception_cause(skipped)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/air/_internal/util.py\", line 88, in run\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     self._ret = self._target(*self._args, **self._kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 115, in <lambda>\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     training_func=lambda: self._trainable_func(self.config),\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 332, in _trainable_func\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     output = fn()\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/tmp/ipykernel_2843/3889220660.py\", line 39, in train_mnist\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m TypeError: init() got an unexpected keyword argument 'tags'\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 733, in dump\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return Pickler.dump(self, obj)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 826, in reducer_override\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     if sys.version_info[:2] < (3, 7) and _is_parametrized_type_hint(\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m RecursionError: maximum recursion depth exceeded in comparison\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2206, in ray._raylet.task_execution_handler\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2102, in ray._raylet.execute_task_with_cancellation_handler\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1756, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1757, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1995, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1085, in ray._raylet.store_task_errors\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 4377, in ray._raylet.CoreWorker.store_task_outputs\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 494, in serialize\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return self._serialize_to_msgpack(value)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 449, in _serialize_to_msgpack\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     value = value.to_bytes()\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/exceptions.py\", line 32, in to_bytes\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     serialized_exception=pickle.dumps(self),\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 88, in dumps\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     cp.dump(obj)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 739, in dump\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise pickle.PicklingError(msg) from e\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m _pickle.PicklingError: Could not pickle object as excessively deep recursion required.\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m An unexpected internal error occurred while the worker was executing a task.\n",
      "\u001b[36m(pid=2778, ip=10.0.114.178)\u001b[0m 2024-01-10 16:27:09.683734: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial train_mnist_20682_00006 started with configuration:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╭───────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ Trial train_mnist_20682_00006 config                          │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ├───────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ hidden                                                    232 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ lr                                                     0.0605 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ momentum                                              0.18335 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ parent_run_id                            ...4a2bb4feb4d87b362 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ threads                                                     2 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╰───────────────────────────────────────────────────────────────╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=3334, ip=10.0.108.154)\u001b[0m 2024-01-10 16:27:11.919836: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[36m(pid=3304, ip=10.0.108.213)\u001b[0m 2024-01-10 16:27:12.038457: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[36m(train_mnist pid=2778, ip=10.0.114.178)\u001b[0m 2024-01-10 16:27:12.131039: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "\u001b[36m(train_mnist pid=2778, ip=10.0.114.178)\u001b[0m 2024-01-10 16:27:12.131073: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[36m(train_mnist pid=2778, ip=10.0.114.178)\u001b[0m 2024-01-10 16:27:12.131090: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ray-659f065bc6aa262f4114e8b6-ray-worker-2): /proc/driver/nvidia/version does not exist\n",
      "\u001b[36m(train_mnist pid=2778, ip=10.0.114.178)\u001b[0m 2024-01-10 16:27:12.131293: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "\u001b[36m(train_mnist pid=2778, ip=10.0.114.178)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(train_mnist pid=2778, ip=10.0.114.178)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "\u001b[36m(train_mnist pid=2778, ip=10.0.114.178)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_mnist pid=2778, ip=10.0.114.178)\u001b[0m 2024/01/10 16:27:12 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of tensorflow. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a supported version, or try upgrading MLflow.\n",
      "\u001b[36m(pid=3506, ip=10.0.100.59)\u001b[0m 2024-01-10 16:27:12.352886: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial task failed for trial train_mnist_20682_00006\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     result = ray.get(future)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/worker.py\", line 2626, in get\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise value\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tclass_name: ImplicitFunc\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tactor_id: 772681920d34573343adb89b16000000\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tpid: 2778\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tnamespace: 53bc2fbb-d6bc-49b8-94e6-19df21a51234\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tip: 10.0.114.178\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None. The worker may have exceeded K8s pod memory limits. Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1807, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1908, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1813, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1754, in ray._raylet.execute_task.function_executor\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise skipped from exception_cause(skipped)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/air/_internal/util.py\", line 88, in run\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     self._ret = self._target(*self._args, **self._kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 115, in <lambda>\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     training_func=lambda: self._trainable_func(self.config),\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 332, in _trainable_func\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     output = fn()\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/tmp/ipykernel_2843/3889220660.py\", line 39, in train_mnist\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m TypeError: init() got an unexpected keyword argument 'tags'\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 733, in dump\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return Pickler.dump(self, obj)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 826, in reducer_override\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     if sys.version_info[:2] < (3, 7) and _is_parametrized_type_hint(\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m RecursionError: maximum recursion depth exceeded in comparison\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2206, in ray._raylet.task_execution_handler\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2102, in ray._raylet.execute_task_with_cancellation_handler\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1756, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1757, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1995, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1085, in ray._raylet.store_task_errors\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 4377, in ray._raylet.CoreWorker.store_task_outputs\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 494, in serialize\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return self._serialize_to_msgpack(value)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 449, in _serialize_to_msgpack\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     value = value.to_bytes()\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/exceptions.py\", line 32, in to_bytes\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     serialized_exception=pickle.dumps(self),\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 88, in dumps\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     cp.dump(obj)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 739, in dump\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise pickle.PicklingError(msg) from e\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m _pickle.PicklingError: Could not pickle object as excessively deep recursion required.\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m An unexpected internal error occurred while the worker was executing a task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial train_mnist_20682_00005 started with configuration:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╭───────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ Trial train_mnist_20682_00005 config                          │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ├───────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ hidden                                                    202 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ lr                                                    0.05866 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ momentum                                               0.7222 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ parent_run_id                            ...4a2bb4feb4d87b362 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ threads                                                     2 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╰───────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial train_mnist_20682_00006 errored after 0 iterations at 2024-01-10 16:27:12. Total running time: 11s\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Error file: /home/ray/ray_results/exp/train_mnist_20682_00006_6_hidden=232,lr=0.0605,momentum=0.1833_2024-01-10_16-27-00/error.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_mnist pid=15682)\u001b[0m 2024-01-10 16:27:12.918491: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "\u001b[36m(train_mnist pid=15682)\u001b[0m 2024-01-10 16:27:12.918525: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[36m(train_mnist pid=15682)\u001b[0m 2024-01-10 16:27:12.918547: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ray-659f065bc6aa262f4114e8b6-ray-head-0): /proc/driver/nvidia/version does not exist\n",
      "\u001b[36m(train_mnist pid=15682)\u001b[0m 2024-01-10 16:27:12.918760: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "\u001b[36m(train_mnist pid=15682)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(train_mnist pid=15682)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "\u001b[36m(train_mnist pid=15682)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_mnist pid=15682)\u001b[0m 2024/01/10 16:27:12 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of tensorflow. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a supported version, or try upgrading MLflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial train_mnist_20682_00005 errored after 0 iterations at 2024-01-10 16:27:13. Total running time: 12s\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Error file: /home/ray/ray_results/exp/train_mnist_20682_00005_5_hidden=202,lr=0.0587,momentum=0.7222_2024-01-10_16-27-00/error.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial task failed for trial train_mnist_20682_00005\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     result = ray.get(future)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/worker.py\", line 2626, in get\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise value\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tclass_name: ImplicitFunc\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tactor_id: d6a169b4b3b26aacd10760c016000000\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tpid: 15682\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tnamespace: 53bc2fbb-d6bc-49b8-94e6-19df21a51234\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tip: 10.0.100.190\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None. The worker may have exceeded K8s pod memory limits. Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1807, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1908, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1813, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1754, in ray._raylet.execute_task.function_executor\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise skipped from exception_cause(skipped)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/air/_internal/util.py\", line 88, in run\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     self._ret = self._target(*self._args, **self._kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 115, in <lambda>\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     training_func=lambda: self._trainable_func(self.config),\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 332, in _trainable_func\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     output = fn()\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/tmp/ipykernel_2843/3889220660.py\", line 39, in train_mnist\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m TypeError: init() got an unexpected keyword argument 'tags'\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 733, in dump\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return Pickler.dump(self, obj)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 826, in reducer_override\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     if sys.version_info[:2] < (3, 7) and _is_parametrized_type_hint(\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m RecursionError: maximum recursion depth exceeded in comparison\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2206, in ray._raylet.task_execution_handler\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2102, in ray._raylet.execute_task_with_cancellation_handler\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1756, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1757, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1995, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1085, in ray._raylet.store_task_errors\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 4377, in ray._raylet.CoreWorker.store_task_outputs\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 494, in serialize\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return self._serialize_to_msgpack(value)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 449, in _serialize_to_msgpack\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     value = value.to_bytes()\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/exceptions.py\", line 32, in to_bytes\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     serialized_exception=pickle.dumps(self),\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 88, in dumps\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     cp.dump(obj)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 739, in dump\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise pickle.PicklingError(msg) from e\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m _pickle.PicklingError: Could not pickle object as excessively deep recursion required.\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m An unexpected internal error occurred while the worker was executing a task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial train_mnist_20682_00007 started with configuration:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╭───────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ Trial train_mnist_20682_00007 config                          │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ├───────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ hidden                                                    227 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ lr                                                    0.04741 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ momentum                                              0.60022 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ parent_run_id                            ...4a2bb4feb4d87b362 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ threads                                                     2 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╰───────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial train_mnist_20682_00008 started with configuration:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╭───────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ Trial train_mnist_20682_00008 config                          │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ├───────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ hidden                                                    394 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ lr                                                    0.03384 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ momentum                                              0.29948 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ parent_run_id                            ...4a2bb4feb4d87b362 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ threads                                                     2 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╰───────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial train_mnist_20682_00009 started with configuration:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╭───────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ Trial train_mnist_20682_00009 config                          │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ├───────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ hidden                                                    343 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ lr                                                    0.06586 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ momentum                                              0.32408 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ parent_run_id                            ...4a2bb4feb4d87b362 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ threads                                                     2 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╰───────────────────────────────────────────────────────────────╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_mnist pid=3334, ip=10.0.108.154)\u001b[0m 2024-01-10 16:27:14.693362: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "\u001b[36m(train_mnist pid=3334, ip=10.0.108.154)\u001b[0m 2024-01-10 16:27:14.693396: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[36m(train_mnist pid=3334, ip=10.0.108.154)\u001b[0m 2024-01-10 16:27:14.693419: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ray-659f065bc6aa262f4114e8b6-ray-worker-4): /proc/driver/nvidia/version does not exist\n",
      "\u001b[36m(train_mnist pid=3334, ip=10.0.108.154)\u001b[0m 2024-01-10 16:27:14.693626: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "\u001b[36m(train_mnist pid=3334, ip=10.0.108.154)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(train_mnist pid=3334, ip=10.0.108.154)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "\u001b[36m(train_mnist pid=3334, ip=10.0.108.154)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_mnist pid=3334, ip=10.0.108.154)\u001b[0m 2024/01/10 16:27:14 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of tensorflow. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a supported version, or try upgrading MLflow.\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial task failed for trial train_mnist_20682_00007\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     result = ray.get(future)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/worker.py\", line 2626, in get\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise value\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tclass_name: ImplicitFunc\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tactor_id: 2543bb02b004131f02b8751e16000000\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tpid: 3334\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tnamespace: 53bc2fbb-d6bc-49b8-94e6-19df21a51234\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tip: 10.0.108.154\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None. The worker may have exceeded K8s pod memory limits. Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1807, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1908, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1813, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1754, in ray._raylet.execute_task.function_executor\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise skipped from exception_cause(skipped)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/air/_internal/util.py\", line 88, in run\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     self._ret = self._target(*self._args, **self._kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 115, in <lambda>\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     training_func=lambda: self._trainable_func(self.config),\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 332, in _trainable_func\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     output = fn()\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/tmp/ipykernel_2843/3889220660.py\", line 39, in train_mnist\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m TypeError: init() got an unexpected keyword argument 'tags'\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 733, in dump\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return Pickler.dump(self, obj)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 826, in reducer_override\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     if sys.version_info[:2] < (3, 7) and _is_parametrized_type_hint(\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m RecursionError: maximum recursion depth exceeded in comparison\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2206, in ray._raylet.task_execution_handler\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2102, in ray._raylet.execute_task_with_cancellation_handler\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1756, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1757, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1995, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1085, in ray._raylet.store_task_errors\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 4377, in ray._raylet.CoreWorker.store_task_outputs\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 494, in serialize\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return self._serialize_to_msgpack(value)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 449, in _serialize_to_msgpack\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     value = value.to_bytes()\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/exceptions.py\", line 32, in to_bytes\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     serialized_exception=pickle.dumps(self),\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 88, in dumps\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     cp.dump(obj)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 739, in dump\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise pickle.PicklingError(msg) from e\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m _pickle.PicklingError: Could not pickle object as excessively deep recursion required.\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m An unexpected internal error occurred while the worker was executing a task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial train_mnist_20682_00007 errored after 0 iterations at 2024-01-10 16:27:14. Total running time: 14s\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Error file: /home/ray/ray_results/exp/train_mnist_20682_00007_7_hidden=227,lr=0.0474,momentum=0.6002_2024-01-10_16-27-00/error.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_mnist pid=3304, ip=10.0.108.213)\u001b[0m 2024-01-10 16:27:14.966095: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "\u001b[36m(train_mnist pid=3304, ip=10.0.108.213)\u001b[0m 2024-01-10 16:27:14.966128: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[36m(train_mnist pid=3304, ip=10.0.108.213)\u001b[0m 2024-01-10 16:27:14.966149: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ray-659f065bc6aa262f4114e8b6-ray-worker-0): /proc/driver/nvidia/version does not exist\n",
      "\u001b[36m(train_mnist pid=3304, ip=10.0.108.213)\u001b[0m 2024-01-10 16:27:14.966357: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "\u001b[36m(train_mnist pid=3304, ip=10.0.108.213)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(train_mnist pid=3304, ip=10.0.108.213)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "\u001b[36m(train_mnist pid=3304, ip=10.0.108.213)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_mnist pid=3304, ip=10.0.108.213)\u001b[0m 2024/01/10 16:27:15 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of tensorflow. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a supported version, or try upgrading MLflow.\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial task failed for trial train_mnist_20682_00008\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     result = ray.get(future)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/worker.py\", line 2626, in get\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise value\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tclass_name: ImplicitFunc\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tactor_id: 9b0aa5961b7571cb3902e58d16000000\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tpid: 3304\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tnamespace: 53bc2fbb-d6bc-49b8-94e6-19df21a51234\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tip: 10.0.108.213\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None. The worker may have exceeded K8s pod memory limits. Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1807, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1908, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1813, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1754, in ray._raylet.execute_task.function_executor\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise skipped from exception_cause(skipped)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/air/_internal/util.py\", line 88, in run\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     self._ret = self._target(*self._args, **self._kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 115, in <lambda>\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     training_func=lambda: self._trainable_func(self.config),\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 332, in _trainable_func\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     output = fn()\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/tmp/ipykernel_2843/3889220660.py\", line 39, in train_mnist\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m TypeError: init() got an unexpected keyword argument 'tags'\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 733, in dump\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return Pickler.dump(self, obj)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 826, in reducer_override\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     if sys.version_info[:2] < (3, 7) and _is_parametrized_type_hint(\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m RecursionError: maximum recursion depth exceeded in comparison\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2206, in ray._raylet.task_execution_handler\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2102, in ray._raylet.execute_task_with_cancellation_handler\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1756, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1757, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1995, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1085, in ray._raylet.store_task_errors\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 4377, in ray._raylet.CoreWorker.store_task_outputs\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 494, in serialize\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return self._serialize_to_msgpack(value)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 449, in _serialize_to_msgpack\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     value = value.to_bytes()\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/exceptions.py\", line 32, in to_bytes\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     serialized_exception=pickle.dumps(self),\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 88, in dumps\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     cp.dump(obj)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 739, in dump\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise pickle.PicklingError(msg) from e\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m _pickle.PicklingError: Could not pickle object as excessively deep recursion required.\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m An unexpected internal error occurred while the worker was executing a task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial train_mnist_20682_00008 errored after 0 iterations at 2024-01-10 16:27:15. Total running time: 14s\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Error file: /home/ray/ray_results/exp/train_mnist_20682_00008_8_hidden=394,lr=0.0338,momentum=0.2995_2024-01-10_16-27-00/error.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_mnist pid=3506, ip=10.0.100.59)\u001b[0m 2024-01-10 16:27:15.181858: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "\u001b[36m(train_mnist pid=3506, ip=10.0.100.59)\u001b[0m 2024-01-10 16:27:15.181892: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[36m(train_mnist pid=3506, ip=10.0.100.59)\u001b[0m 2024-01-10 16:27:15.181909: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ray-659f065bc6aa262f4114e8b6-ray-worker-1): /proc/driver/nvidia/version does not exist\n",
      "\u001b[36m(train_mnist pid=3506, ip=10.0.100.59)\u001b[0m 2024-01-10 16:27:15.182116: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "\u001b[36m(train_mnist pid=3506, ip=10.0.100.59)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(train_mnist pid=3506, ip=10.0.100.59)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "\u001b[36m(train_mnist pid=3506, ip=10.0.100.59)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_mnist pid=3506, ip=10.0.100.59)\u001b[0m 2024/01/10 16:27:15 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of tensorflow. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a supported version, or try upgrading MLflow.\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial task failed for trial train_mnist_20682_00009\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     result = ray.get(future)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/worker.py\", line 2626, in get\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise value\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tclass_name: ImplicitFunc\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tactor_id: 1b3c9d325297960480ac2f3816000000\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tpid: 3506\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tnamespace: 53bc2fbb-d6bc-49b8-94e6-19df21a51234\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \tip: 10.0.100.59\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code None. The worker may have exceeded K8s pod memory limits. Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1807, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1908, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1813, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1754, in ray._raylet.execute_task.function_executor\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise skipped from exception_cause(skipped)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/air/_internal/util.py\", line 88, in run\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     self._ret = self._target(*self._args, **self._kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 115, in <lambda>\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     training_func=lambda: self._trainable_func(self.config),\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 467, in _resume_span\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 332, in _trainable_func\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     output = fn()\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/tmp/ipykernel_2843/3889220660.py\", line 39, in train_mnist\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m TypeError: init() got an unexpected keyword argument 'tags'\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 733, in dump\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return Pickler.dump(self, obj)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 826, in reducer_override\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     if sys.version_info[:2] < (3, 7) and _is_parametrized_type_hint(\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m RecursionError: maximum recursion depth exceeded in comparison\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2206, in ray._raylet.task_execution_handler\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2102, in ray._raylet.execute_task_with_cancellation_handler\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1756, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1757, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1995, in ray._raylet.execute_task\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1085, in ray._raylet.store_task_errors\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"python/ray/_raylet.pyx\", line 4377, in ray._raylet.CoreWorker.store_task_outputs\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 494, in serialize\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     return self._serialize_to_msgpack(value)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 449, in _serialize_to_msgpack\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     value = value.to_bytes()\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/exceptions.py\", line 32, in to_bytes\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     serialized_exception=pickle.dumps(self),\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 88, in dumps\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     cp.dump(obj)\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 739, in dump\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m     raise pickle.PicklingError(msg) from e\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m _pickle.PicklingError: Could not pickle object as excessively deep recursion required.\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m An unexpected internal error occurred while the worker was executing a task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial train_mnist_20682_00009 errored after 0 iterations at 2024-01-10 16:27:15. Total running time: 14s\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Error file: /home/ray/ray_results/exp/train_mnist_20682_00009_9_hidden=343,lr=0.0659,momentum=0.3241_2024-01-10_16-27-00/error.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 00:27:17,709\tWARNING experiment_analysis.py:584 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trial status: 10 ERROR\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Current time: 2024-01-10 16:27:17. Total running time: 17s\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Logical resource usage: 0/6 CPUs, 0/0 GPUs\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╭────────────────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ Trial name                status            lr     momentum     hidden │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ├────────────────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00000   ERROR      0.044958      0.164275        457 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00001   ERROR      0.0531664     0.551038        142 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00002   ERROR      0.0483602     0.860322        168 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00003   ERROR      0.0819864     0.769916        119 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00004   ERROR      0.0329543     0.512749        410 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00005   ERROR      0.0586611     0.722196        202 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00006   ERROR      0.0604956     0.183346        232 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00007   ERROR      0.0474074     0.600219        227 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00008   ERROR      0.0338391     0.29948         394 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00009   ERROR      0.0658625     0.324079        343 │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╰────────────────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Number of errored trials: 10\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ Trial name                  # failures   error file                                                                                                             │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ├─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00000              1   /home/ray/ray_results/exp/train_mnist_20682_00000_0_hidden=457,lr=0.0450,momentum=0.1643_2024-01-10_16-27-00/error.txt │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00001              1   /home/ray/ray_results/exp/train_mnist_20682_00001_1_hidden=142,lr=0.0532,momentum=0.5510_2024-01-10_16-27-00/error.txt │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00002              1   /home/ray/ray_results/exp/train_mnist_20682_00002_2_hidden=168,lr=0.0484,momentum=0.8603_2024-01-10_16-27-00/error.txt │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00003              1   /home/ray/ray_results/exp/train_mnist_20682_00003_3_hidden=119,lr=0.0820,momentum=0.7699_2024-01-10_16-27-00/error.txt │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00004              1   /home/ray/ray_results/exp/train_mnist_20682_00004_4_hidden=410,lr=0.0330,momentum=0.5127_2024-01-10_16-27-00/error.txt │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00005              1   /home/ray/ray_results/exp/train_mnist_20682_00005_5_hidden=202,lr=0.0587,momentum=0.7222_2024-01-10_16-27-00/error.txt │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00006              1   /home/ray/ray_results/exp/train_mnist_20682_00006_6_hidden=232,lr=0.0605,momentum=0.1833_2024-01-10_16-27-00/error.txt │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00007              1   /home/ray/ray_results/exp/train_mnist_20682_00007_7_hidden=227,lr=0.0474,momentum=0.6002_2024-01-10_16-27-00/error.txt │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00008              1   /home/ray/ray_results/exp/train_mnist_20682_00008_8_hidden=394,lr=0.0338,momentum=0.2995_2024-01-10_16-27-00/error.txt │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m │ train_mnist_20682_00009              1   /home/ray/ray_results/exp/train_mnist_20682_00009_9_hidden=343,lr=0.0659,momentum=0.3241_2024-01-10_16-27-00/error.txt │\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m ╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Experiment checkpoint syncing has been triggered multiple times in the last 30.0 seconds. A sync will be triggered whenever a trial has checkpointed more than `num_to_keep` times since last sync or if 300 seconds have passed since last sync. If you have set `num_to_keep` in your `CheckpointConfig`, consider increasing the checkpoint frequency or keeping more checkpoints. You can supress this warning by changing the `TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S` environment variable.\n",
      "\u001b[36m(TunerInternal pid=15510)\u001b[0m Trials did not complete: [train_mnist_20682_00000, train_mnist_20682_00001, train_mnist_20682_00002, train_mnist_20682_00003, train_mnist_20682_00004, train_mnist_20682_00005, train_mnist_20682_00006, train_mnist_20682_00007, train_mnist_20682_00008, train_mnist_20682_00009]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No best trial found for the given metric: mean_accuracy. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m parent_run_id \u001b[38;5;241m=\u001b[39m run\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mrun_id\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(parent_run_id)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtune_mnist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent_run_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 81\u001b[0m, in \u001b[0;36mtune_mnist\u001b[0;34m(parent_run_id)\u001b[0m\n\u001b[1;32m     58\u001b[0m tuner \u001b[38;5;241m=\u001b[39m tune\u001b[38;5;241m.\u001b[39mTuner(\n\u001b[1;32m     59\u001b[0m     tune\u001b[38;5;241m.\u001b[39mwith_resources(train_mnist, resources\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m}),\n\u001b[1;32m     60\u001b[0m     tune_config\u001b[38;5;241m=\u001b[39mtune\u001b[38;5;241m.\u001b[39mTuneConfig(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m     },\n\u001b[1;32m     78\u001b[0m )\n\u001b[1;32m     79\u001b[0m results \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mfit()\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters found were: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconfig)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/ray/tune/result_grid.py:162\u001b[0m, in \u001b[0;36mResultGrid.get_best_result\u001b[0;34m(self, metric, mode, scope, filter_nan_and_inf)\u001b[0m\n\u001b[1;32m    151\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo best trial found for the given metric: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment_analysis\u001b[38;5;241m.\u001b[39mdefault_metric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis means that no trial has reported this metric\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m     )\n\u001b[1;32m    156\u001b[0m     error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, or all values reported for this metric are NaN. To not ignore NaN \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues, you can set the `filter_nan_and_inf` arg to False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filter_nan_and_inf\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error_msg)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trial_to_result(best_trial)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No best trial found for the given metric: mean_accuracy. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False."
     ]
    }
   ],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    parent_run_id = run.info.run_id\n",
    "    print(parent_run_id)\n",
    "    tune_mnist(parent_run_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b79dc-b712-4695-b6a9-88ab3ec96547",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install tblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12954385-aa8e-4892-a9d4-226e6603c00e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c544e15c-6a09-4915-b576-9462ac6b9ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dca-init": "true",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
